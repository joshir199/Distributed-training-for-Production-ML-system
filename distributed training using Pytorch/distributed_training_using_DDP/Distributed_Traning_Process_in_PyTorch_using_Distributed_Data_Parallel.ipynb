{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Understanding Distributed Data Parallel (DDP) of Pytorch\n",
        "\"\"\"\n",
        "DDP implements data parallelism at the module level which can run across multiple machines.\n",
        "Data parallelism is a way to process multiple data batches across multiple devices\n",
        "simultaneously to achieve better performance. In PyTorch, the DistributedSampler\n",
        "ensures each device gets a non-overlapping input batch. The model is replicated on\n",
        "all the devices; each replica calculates gradients and simultaneously synchronizes\n",
        "with the others using the ring all-reduce algorithm.\n",
        "\"\"\"\n",
        "\n",
        "# Note: Try to use this notebook with multiple GPUs only\n"
      ],
      "metadata": {
        "id": "VZZySb8Yp3OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G2Vf8ad7wdsF"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "736Bwn8Spv4C",
        "outputId": "25f2d2d1-dadc-43de-d8d3-b77152873f57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank =2\n",
        "map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
        "map_location"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGOkge0s_xDE",
        "outputId": "534dac60-af1d-41dd-8ef1-5d168287ca7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cuda:0': 'cuda:2'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets understand how multiple GPU devices are detected and computation are done on it.\n",
        "# Imagine we have 2 GPU system\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)\n",
        "if torch.cuda.device_count() == 2:\n",
        "  device0 = torch.device('cuda:0')\n",
        "  device1 = torch.device('cuda:1')\n",
        "  print(device1)\n",
        "\n",
        "  x = torch.tensor([2., 3.], device=device) # takes the default GPU device\n",
        "  y = torch.tensor([4., 7.], device = device0) # takes specific GPU device mentioned\n",
        "\n",
        "  # Context GPU\n",
        "  with torch.cuda.device(1):\n",
        "\n",
        "    a = torch.tensor([9., 6.], device= device) # It takes the default context GPU\n",
        "    b = torch.tensor([9., 6.]).cuda() #  It also takes the default context GPU\n",
        "\n",
        "    c = a + b  # operations will be done on context GPU\n",
        "    \"\"\"\n",
        "    Note: Cross-GPU operations are not allowed by default. If two elements are on\n",
        "    two different device, computation can't be done.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "yioSUfatpyfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.distributed leverages message passing semantics allowing each process to communicate data\n",
        "# to any of the other processes. As opposed to the multiprocessing (torch.multiprocessing)\n",
        "# package, processes can use different communication backends and are not restricted to\n",
        "# being executed on the same machine.\n",
        "# It supports three built-in backends, each with different capabilities.\n",
        "\n",
        "\"\"\"\n",
        "Rule of thumb\n",
        "\n",
        " -> Use the NCCL backend for distributed GPU training\n",
        "\n",
        " -> Use the Gloo backend for distributed CPU training.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Backends for torch.distributed\n",
        "\n",
        "# 1. Gloo\n",
        "\"\"\"\n",
        " It supports all point-to-point and collective operations on CPU, and all collective\n",
        " operations on GPU. The implementation of the collective operations for CUDA tensors\n",
        " is not as optimized as the ones provided by the NCCL backend.\n",
        "\n",
        " In order to use multiple GPUs, let us also make the following modifications:\n",
        "\n",
        " 1. Use device = torch.device(\"cuda:{}\".format(rank))\n",
        "\n",
        " 2. model = Net() →→ model = Net().to(device)\n",
        "\n",
        " 3. Use data, target = data.to(device), target.to(device\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 2. NCCL Backend\n",
        "\"\"\"\n",
        " It provides an optimized implementation of collective operations against CUDA tensors.\n",
        " If you only use CUDA tensors for your collective operations, consider using this\n",
        " backend for the best in class performance\n",
        "\n",
        " It needs to be initialized using the torch.distributed.init_process_group()\n",
        " function before calling any other methods.\n",
        " \"\"\"\n"
      ],
      "metadata": {
        "id": "Iy0aufnavRb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Processes for NCCL backend DDP\n",
        "\n",
        "# Returns True if the distributed package is available.\n",
        "torch.distributed.is_available()\n",
        "\n",
        "#\n",
        "torch.distributed.init_process_group(*args, **kwargs)\n",
        "\"\"\"\n",
        "Initializes the default distributed process group, and this will also initialize the distributed package.\n",
        "\n",
        "There are 2 main ways to initialize a process group:\n",
        " 1. Specify store, rank, and world_size explicitly.\n",
        "\n",
        " 2. Specify init_method (a URL string) which indicates where/how to discover peers.\n",
        "    Optionally specify rank and world_size, or encode all required parameters in\n",
        "    the URL and omit them.\n",
        "\n",
        "If neither is specified, init_method is assumed to be “env://”.\n",
        "\n",
        "\"\"\"\n",
        "# world_size = Number of processes participating in the job. Required if store is specified.\n",
        "# rank = Rank of the current process (it should be a number between 0 and world_size-1).\n",
        "#         Required if store is specified.\n",
        "\n",
        "# For more refernece, visit :  https://pytorch.org/docs/stable/distributed.html#module-torch.distributed\n"
      ],
      "metadata": {
        "id": "9gtYHOwoyfhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the environment and initialisating process_group\n",
        "def setup(rank, world_size):\n",
        "\n",
        "  # ************ important step\n",
        "  os.environ['MASTER_ADDR'] = 'localhost'\n",
        "  os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "  #************** important step\n",
        "  # initialize the process group\n",
        "  dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "#***************\n",
        "# Destroying process group created for training comupation at the end\n",
        "def cleanup():\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "\n",
        "#**************\n",
        "# Define Simple Linear model for demo purpose\n",
        "class SimpleLinearModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SimpleLinearModel, self).__init__()\n",
        "    self.net = nn.Linear(10, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "rSXFE6Hb0XyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training step using DDP methodology\n",
        "def demo_training(rank, world_size):\n",
        "\n",
        "  print(f\"Running basic DDP example on rank {rank}.\")\n",
        "  setup(rank, world_size)\n",
        "\n",
        "  # create model and move it to GPU with id rank\n",
        "  model = SimpleLinearModel().to(rank)\n",
        "  #******* Important step\n",
        "  ddp_model = DDP(model, device_ids=[rank])\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
        "\n",
        "  # DistributedSampler chunks the input data across all distributed processes.\n",
        "  # the effective batch size is 32 * nprocs\n",
        "  \"\"\"\n",
        "  train_data = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    sampler=DistributedSampler(train_dataset)\n",
        "  \"\"\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = ddp_model(torch.randn(20, 10))\n",
        "  labels = torch.randn(20, 10).to(rank)\n",
        "  loss_fn(outputs, labels).backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  cleanup()"
      ],
      "metadata": {
        "id": "V6ELImGU2dxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code to spawn model on each processor before starting training\n",
        "def run_demo(demo_fn, world_size):\n",
        "\n",
        "  mp.spawn(demo_fn,args=(world_size,),nprocs=world_size,join=True)"
      ],
      "metadata": {
        "id": "lz9184qO36sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: DDP won't run in Jupyter Notebook, It needs to be run as a Python script.\n",
        "# Please refer to the Python script and training model added in this repo.\n"
      ],
      "metadata": {
        "id": "J79O_ePypzor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Issues with the DDP training methodology\n",
        "\n",
        "# Skewed Processing Speeds\n",
        "\"\"\"\n",
        "Different processes are expected to launch the same number of synchronizations and\n",
        "reach these synchronization points in the same order and enter each synchronization\n",
        "point at roughly the same time. Otherwise, fast processes might arrive early and\n",
        "timeout while waiting for stragglers.\n",
        "\"\"\"\n",
        "# Note: Developers are responsible for managing and monitoring balance workload\n",
        "# distributions across processes.\n",
        "\n",
        "\"\"\"\n",
        "Sometimes, skewed processing speeds are inevitable due to, e.g., network delays,\n",
        "resource contentions, or unpredictable workload spikes.\n",
        "To avoid timeouts in these situations, make sure that you pass a sufficiently large\n",
        "timeout value when calling init_process_group.\n",
        "-> timeout=datetime.timedelta(seconds=1800) (by default, it is 30 minutes)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tcoPYtDO5OXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and Loading Checkpoints\n",
        "\"\"\"\n",
        "It's common practice to use torch.save and torch.load to checkpoint modules during\n",
        "training and recover from checkpoints.\n",
        "\n",
        "When using DDP, one optimization is to save the model in only one process(rank 0),then\n",
        "load it to all processes, reducing write overhead. This is correct because all processes\n",
        "start from the same parameters and gradients are synchronized in backward passes,\n",
        "and hence optimizers should keep setting parameters to the same values.\n",
        "\"\"\"\n",
        "# Note: Make sure no process starts loading before the saving is finished. To achieve\n",
        "# this, we can ad barrier before starting next model update cycle.\n",
        "# torch.distributed.barrier()\n",
        "\n",
        "\"\"\"\n",
        "Additionally, when loading the module, you need to provide an appropriate map_location\n",
        "argument to prevent a process from stepping into others’ devices. If map_location\n",
        "is missing, torch.load will first load the module to CPU.\n",
        "\"\"\"\n",
        "# Note: For more advanced failure recovery and elasticity support, refer to TorchElastic."
      ],
      "metadata": {
        "id": "74WDvPMe6TuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "# DDP model training using Checkpoints as fault-tolerance procedure\n",
        "\n",
        "def demo_training_with_checkpoint(rank, world_size):\n",
        "\n",
        "  print(f\"Running training with checkpoint example on rank {rank}.\")\n",
        "  setup(rank, world_size)\n",
        "\n",
        "  # create model and move it to GPU with id rank\n",
        "  model = SimpleLinearModel().to(rank)\n",
        "  #******* Important step\n",
        "  ddp_model = DDP(model, device_ids=[rank])\n",
        "\n",
        "  # &&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "  CHK_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
        "  if rank==0:\n",
        "    torch.save(ddp_model.state_dict(), CHK_PATH)\n",
        "\n",
        "  # Use a barrier() to make sure that other process loads the model after process 0 saves it.\n",
        "  dist.barrier()\n",
        "\n",
        "  # configure map_location properly, e.g: {'cuda:0': 'cuda:2'} for rank = 2\n",
        "  map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
        "\n",
        "  ddp_model.load_state_dict(torch.load(CHK_PATH, map_location=map_location))\n",
        "  # &&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
        "\n",
        "  # DistributedSampler chunks the input data across all distributed processes.\n",
        "  # the effective batch size is 32 * nprocs\n",
        "  \"\"\"\n",
        "  train_data = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    sampler=DistributedSampler(train_dataset)\n",
        "  \"\"\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = ddp_model(torch.randn(20, 10))\n",
        "  labels = torch.randn(20, 10).to(rank)\n",
        "  loss_fn(outputs, labels).backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  cleanup()"
      ],
      "metadata": {
        "id": "5tJ8qW6t-ZIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z5lj6D7-7T4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}